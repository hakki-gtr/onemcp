# Application defaults for OneMCP (HTTP Streamable transport over Spring WebFlux)
#
# Notes for operators:
# - server.port controls the HTTP port. Override via SERVER_PORT or Spring's standard env mapping.
# - agentProtocol.mcp.config.* controls the MCP transport endpoint characteristics.
# - Provider credentials are read from environment variables; leave blank in source control.

server:
  port: 8080
  # servlet context path can be set via `server.servlet.context-path` if needed
  # servlet:
  #   context-path: /api

agentProtocol:
  mcp:
    config:
      # If true, the transport will reject HTTP DELETE requests (defense-in-depth for some proxies)
      disallowDelete: ${PROTOCOL_MCP_CONFIG_DISALLOW_DELETE:false}
      # Path where the MCP streamable endpoint is exposed (combined with server.port and optional context-path)
      messageEndpoint: ${PROTOCOL_MCP_CONFIG_MESSAGE_ENDPOINT:/mcp}

providers:
  providers:
    openai:
      apiKey: ${OPENAI_API_KEY:}
      baseUrl: ${OPENAI_BASE_URL:}
      modelName: ${OPENAI_MODEL_NAME:gpt-5-mini-2025-08-07}
    gemini:
      apiKey: ${GEMINI_API_KEY:}
      endpoint: ${GEMINI_ENDPOINT:}
      modelName: ${GEMINI_MODEL_NAME:gemini-2.0-flash}
    anthropic:
      apiKey: ${ANTHROPIC_API_KEY:}
      baseUrl: ${ANTHROPIC_BASE_URL:}
      modelName: ${ANTHROPIC_MODEL_NAME:claude-3-sonnet-20240229}
  # Default inference provider key from the list above
  default-provider: ${INFERENCE_DEFAULT_PROVIDER:openai}

# URL for the external TypeScript runtime used by some tools
typescriptRuntime:
  baseUrl: ${TS_RUNTIME_URL:http://localhost:7070}

# Guardrails configuration
guardrails:
  strict: ${GUARDRAILS_STRICT:true}

# OpenTelemetry service naming
otel:
  service:
    name: onemcp
  exporter:
    otlp:
      endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT:http://127.0.0.1:4317}


knowledgeBase:
  # Root Foundation directory containing docs/, tests/, feedback/, and openapi/ subfolders.
  # Override with env var FOUNDATION_DIR or set knowledgeBase.foundation.dir
  foundation:
    dir: ${FOUNDATION_DIR:/var/foundation}
  # Hint generation settings for KB entries
  hint:
    # Max characters used when generating hints (also caps AI output)
    size: 500
    # If true, uses the configured InferenceService to generate a concise hint for each entry.
    # Otherwise, falls back to the first N characters of the content.
    useAi: true
